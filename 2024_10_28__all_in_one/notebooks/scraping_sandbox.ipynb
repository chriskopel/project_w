{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The goal here is to input any team, search through a list, and return that team rather than having disparate classes for each one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soccer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_data = pd.read_csv(r\"C:\\Users\\Owner\\Documents\\Data Projects\\GitHub\\Apps\\project_w\\data\\football_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team = 'Aston Villa'\n",
    "team_url = fb_data[fb_data['team'] == team].url.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ChromeDriver path from your environment variable\n",
    "chrome_driver_path = os.getenv('chrome_driver_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    # Setup WebDriver\n",
    "    service = Service(chrome_driver_path)  # Use the path from environment variable\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    # Open the page\n",
    "    driver.get(team_url)\n",
    "    \n",
    "    # Wait until the cookie popup is present and clickable\n",
    "    WebDriverWait(driver, 5).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//button[text()='Reject All']\"))\n",
    "    ).click()\n",
    "\n",
    "    # Now proceed with your scraping task\n",
    "    ## Use BeautifulSoup to parse the page source once the page is fully loaded\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Find the table and extract data as before\n",
    "    table = soup.find_all('table', class_='matches')\n",
    "    dates = [row.text for row in table[0].find_all('td', class_=\"full-date\")]\n",
    "    leagues = [row.text.strip() for row in table[0].find_all('td', class_=\"competition\")]\n",
    "    homes = [row.text.strip() for row in table[0].find_all('td', class_=\"team\")[::2]]\n",
    "    aways = [row.text.strip() for row in table[0].find_all('td', class_=\"team\")[1::2]]\n",
    "    times = [row.text.strip() for row in table[0].find_all('td', class_=\"score-time\")]\n",
    "\n",
    "    # Create dataframes\n",
    "    df_fixtures = pd.DataFrame(\n",
    "        {\n",
    "            'Date': dates,\n",
    "            'League': leagues,\n",
    "            'Home team': homes,\n",
    "            'Time': times,\n",
    "            'Away team': aways\n",
    "        }\n",
    "    )\n",
    "\n",
    "except TimeoutException:\n",
    "    print(\"Timed out waiting for cookie pop-up or other elements\")\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the matches tab scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_url_m = team_url + 'matches/'\n",
    "team_url_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ChromeDriver path from your environment variable\n",
    "chrome_driver_path = os.getenv('chrome_driver_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    # Setup WebDriver\n",
    "    service = Service(chrome_driver_path)  # Use the path from environment variable\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    # Open the page\n",
    "    driver.get(team_url_m)\n",
    "    \n",
    "    # Wait until the cookie popup is present and clickable\n",
    "    WebDriverWait(driver, 5).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//button[text()='Reject All']\"))\n",
    "    ).click()\n",
    "\n",
    "    # Now proceed with your scraping task\n",
    "    ## Use BeautifulSoup to parse the page source once the page is fully loaded\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Find the table and extract data as before\n",
    "    table = soup.find_all('table', class_='matches')\n",
    "    dates = [row.text for row in table[0].find_all('td', class_=\"full-date\")]\n",
    "    leagues = [row.text.strip() for row in table[0].find_all('td', class_=\"competition\")]\n",
    "    homes = [row.text.strip() for row in table[0].find_all('td', class_=\"team\")[::2]]\n",
    "    aways = [row.text.strip() for row in table[0].find_all('td', class_=\"team\")[1::2]]\n",
    "    times = [row.text.strip() for row in table[0].find_all('td', class_=\"score-time\")]\n",
    "\n",
    "    # Create dataframes\n",
    "    df_fixtures = pd.DataFrame(\n",
    "        {\n",
    "            'Date': dates,\n",
    "            'League': leagues,\n",
    "            'Home team': homes,\n",
    "            'Time': times,\n",
    "            'Away team': aways\n",
    "        }\n",
    "    )\n",
    "\n",
    "except TimeoutException:\n",
    "    print(\"Timed out waiting for cookie pop-up or other elements\")\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fixtures.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through each soccer team for df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dictionary\n",
    "team_url_dict = fb_data.set_index('team')['url'].to_dict()\n",
    "\n",
    "# Amend each URL by adding 'matches/' at the end\n",
    "team_url_dict = {team: url + 'matches/' for team, url in team_url_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the first three key-value pairs\n",
    "# first_three = {k: team_url_dict[k] for k in list(team_url_dict.keys())[:3]}\n",
    "# first_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ChromeDriver path from your environment variable\n",
    "chrome_driver_path = os.getenv('chrome_driver_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for team, url in team_url_dict.items():\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Setup WebDriver\n",
    "        service = Service(chrome_driver_path)  # Use the path from environment variable\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "\n",
    "        # Open the page\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait until the cookie popup is present and clickable\n",
    "        WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[text()='Reject All']\"))\n",
    "        ).click()\n",
    "\n",
    "        # Now proceed with your scraping task\n",
    "        ## Use BeautifulSoup to parse the page source once the page is fully loaded\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Find the table and extract data as before\n",
    "        table = soup.find_all('table', class_='matches')\n",
    "        dates = [row.text for row in table[0].find_all('td', class_=\"full-date\")]\n",
    "        leagues = [row.text.strip() for row in table[0].find_all('td', class_=\"competition\")]\n",
    "        homes = [row.text.strip() for row in table[0].find_all('td', class_=\"team\")[::2]]\n",
    "        aways = [row.text.strip() for row in table[0].find_all('td', class_=\"team\")[1::2]]\n",
    "        times = [row.text.strip() for row in table[0].find_all('td', class_=\"score-time\")]\n",
    "\n",
    "        # Create dataframes\n",
    "        df_team_fixtures = pd.DataFrame(\n",
    "            {\n",
    "                'Team': team,\n",
    "                'Date': dates,\n",
    "                'League': leagues,\n",
    "                'Home Team': homes,\n",
    "                'Time': times,\n",
    "                'Away Team': aways\n",
    "            }\n",
    "        )\n",
    "\n",
    "        ## Add to master df\n",
    "        if 'df_fb_master' in locals():\n",
    "            df_fb_master = pd.concat([df_fb_master, df_team_fixtures], ignore_index=True)\n",
    "        else:\n",
    "            df_fb_master = df_team_fixtures.copy()\n",
    "\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for cookie pop-up or other elements\")\n",
    "        \n",
    "    finally:\n",
    "        # Close the browser\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fb_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fb_master.Team.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fb_master.groupby('Team').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fb_master.to_csv('df_fb_master_2024_10_28.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_abbr_data = pd.read_csv(r\"C:\\Users\\Owner\\Documents\\Data Projects\\GitHub\\Apps\\project_w\\data\\nba_team_abbr.csv\")\n",
    "today = datetime.now().date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init helper fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply the condition\n",
    "def get_game_time_status(date):\n",
    "    if date.date() >= today:\n",
    "        return 'Upcoming'\n",
    "    else:\n",
    "        return 'Past'\n",
    "    \n",
    "# Define a function to apply the condition\n",
    "def get_season(row_number):\n",
    "    if row_number >= 82:\n",
    "        return 'playoffs'\n",
    "    else:\n",
    "        return 'regular season'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build scraper fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the schedule from nba ref\n",
    "def retrieve_schedule(team_abbr):\n",
    "\n",
    "    # URL of the website\n",
    "    url = f\"https://www.basketball-reference.com/teams/{team_abbr}/2025_games.html\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"YourAppName/1.0 (https://yourwebsite.com; contact@yourwebsite.com)\"\n",
    "    }\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        time.sleep(4) # if successful, wait n secs before reading in the html\n",
    "        dfs_list = pd.read_html(url)\n",
    "\n",
    "        # Prep data\n",
    "        drop_cols = ['Unnamed: 3','Unnamed: 4']\n",
    "        col_mapping = {\n",
    "            'Unnamed: 5': 'Home/Away',\n",
    "            'Unnamed: 7': 'Result',\n",
    "            'Unnamed: 8': 'OT'\n",
    "        }\n",
    "\n",
    "        # Organize\n",
    "        df = pd.concat(dfs_list, ignore_index=True).drop(columns=drop_cols).rename(columns=col_mapping)\n",
    "        df['Home/Away'] = df['Home/Away'].fillna(\"vs.\")\n",
    "        df = df[df['Date'] != 'Date'].reset_index(drop=True)\n",
    "        df['computer_date'] = pd.to_datetime(df['Date'], format='%a, %b %d, %Y')\n",
    "        df['user_team'] = nba_abbr_data['team'][nba_abbr_data['abbr'] == team_abbr].iloc[0]\n",
    "        \n",
    "        # Create new cols from fns\n",
    "        df['GameTimeStatus'] = df['computer_date'].apply(lambda x: get_game_time_status(x))\n",
    "        df['game_type'] = df.index.to_series().apply(lambda x: get_season(x))\n",
    "        df['game_type'] = np.where(df['Notes'] == 'In-Season Tournament', 'In-Season Tournament', df['game_type']) # Apply conditional update to game_type column\n",
    "\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        print(f\"Error retrieving data for {team_abbr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_html(\"https://www.basketball-reference.com/teams/ATL/2025_games.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through each nba team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nba_team in nba_abbr_data['abbr']:\n",
    "    \n",
    "    # Scrape\n",
    "    nba_team_sch = retrieve_schedule(nba_team)\n",
    "\n",
    "    # Check for dataframe\n",
    "    if 'df_nba_master' in locals():\n",
    "        df_nba_master = pd.concat([df_nba_master, nba_team_sch], ignore_index=True)\n",
    "    else:\n",
    "        df_nba_master = nba_team_sch.copy()\n",
    "\n",
    "    # Pause for n seconds to address HTTPError: HTTP Error 429: Too Many Requests\n",
    "    time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_nba_master.user_team.value_counts(), len(df_nba_master.user_team.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pho_team_sch = retrieve_schedule('PHO')\n",
    "# df_nba_master = pd.concat([df_nba_master, pho_team_sch], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nba_master.to_csv('df_nba_master_2024_10_29.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NHL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhl_abbr_data = pd.read_csv(r\"C:\\Users\\Owner\\Documents\\Data Projects\\GitHub\\Apps\\project_w\\data\\nhl_team_abbr.csv\")\n",
    "today = datetime.now().date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init helper fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply the condition\n",
    "def get_game_time_status(date):\n",
    "    if date.date() >= today:\n",
    "        return 'Upcoming'\n",
    "    else:\n",
    "        return 'Past'\n",
    "    \n",
    "# Define a function to apply the condition\n",
    "def get_season(row_number):\n",
    "    if row_number >= 82:\n",
    "        return 'playoffs'\n",
    "    else:\n",
    "        return 'regular season'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build scraper fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the schedule from nba ref\n",
    "def retrieve_nhl_schedule(team_abbr):\n",
    "\n",
    "    # URL of the website\n",
    "    url = f\"https://www.hockey-reference.com/teams/{team_abbr}/2025_games.html\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"YourAppName/1.0 (https://yourwebsite.com; contact@yourwebsite.com)\"\n",
    "    }\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        time.sleep(4) # if successful, wait n secs before reading in the html\n",
    "        dfs_list = pd.read_html(url)\n",
    "\n",
    "        # Prep data\n",
    "        col_mapping = {\n",
    "            'Unnamed: 3': 'Home/Away',\n",
    "            'Unnamed: 7': 'Result',\n",
    "            'Unnamed: 8': 'OT'\n",
    "        }\n",
    "\n",
    "        # Organize\n",
    "        df = pd.concat(dfs_list, ignore_index=True).rename(columns=col_mapping)\n",
    "        df['Home/Away'] = df['Home/Away'].fillna(\"vs.\")\n",
    "        df = df[df['Date'] != 'Date'].reset_index(drop=True)\n",
    "        df['computer_date'] = pd.to_datetime(df['Date'])\n",
    "        df['Date'] = df['computer_date'].dt.strftime('%a, %b %d, %Y')\n",
    "        df['user_team'] = nhl_abbr_data['team'][nhl_abbr_data['abbr'] == team_abbr].iloc[0]\n",
    "        \n",
    "        # Create new cols from fns\n",
    "        df['GameTimeStatus'] = df['computer_date'].apply(lambda x: get_game_time_status(x))\n",
    "        df['game_type'] = df.index.to_series().apply(lambda x: get_season(x))\n",
    "\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        print(f\"Error retrieving data for {team_abbr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through each nhl team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nhl_team in nhl_abbr_data['abbr']:\n",
    "    \n",
    "    # Scrape\n",
    "    nhl_team_sch = retrieve_nhl_schedule(nhl_team)\n",
    "\n",
    "    # Check for dataframe\n",
    "    if 'df_nhl_master' in locals():\n",
    "        df_nhl_master = pd.concat([df_nhl_master, nhl_team_sch], ignore_index=True)\n",
    "    else:\n",
    "        df_nhl_master = nhl_team_sch.copy()\n",
    "\n",
    "    # Pause for n seconds to address HTTPError: HTTP Error 429: Too Many Requests\n",
    "    time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_nhl_master.user_team.value_counts(), len(df_nhl_master.user_team.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veg_team_sch = retrieve_nhl_schedule('VEG')\n",
    "# df_nhl_master = pd.concat([df_nhl_master, veg_team_sch], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nhl_master.to_csv('df_nhl_master_2024_10_30.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nhl_master[(df_nhl_master['user_team'] == 'Colorado Avalanche') & (df_nhl_master['GameTimeStatus'] == 'Upcoming')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NFL (reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import time\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_abbr_data = pd.read_csv(r\"C:\\Users\\Owner\\Documents\\Data Projects\\GitHub\\Apps\\project_w\\data\\nfl_team_abbr.csv\")\n",
    "today = datetime.now().date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build scraper fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the schedule from nba ref\n",
    "def retrieve_nfl_schedule(team_abbr):\n",
    "\n",
    "    # URL of the website\n",
    "    url = f\"https://www.pro-football-reference.com/teams/{team_abbr}/2024/gamelog/\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"YourAppName/1.0 (https://yourwebsite.com; contact@yourwebsite.com)\"\n",
    "    }\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        time.sleep(4) # if successful, wait n secs before reading in the html\n",
    "        dfs_list = pd.read_html(url)\n",
    "\n",
    "        # Concatenate all DataFrames in dfs_list\n",
    "        df = pd.concat(dfs_list, ignore_index=True)\n",
    "\n",
    "        # Flatten the multi-level columns, ignoring 'Unnamed' and removing '_level_'\n",
    "        df.columns = [\n",
    "            re.sub(r'\\d+\\s', '', ' '.join(col).strip().replace(\"Unnamed: \", \"\").replace(\"_level_\", \"\")).replace(\"0\", \"\").replace(\"1\", \"\").replace(\"6\",\"Home/Away\").replace(\"4\",\"Result\")\n",
    "            for col in df.columns\n",
    "        ]\n",
    "\n",
    "        df = df.drop(columns='3')\n",
    "\n",
    "        # Organize\n",
    "        df['Home/Away'] = df['Home/Away'].fillna(\"vs.\")\n",
    "        df['user_team'] = nfl_abbr_data['team'][nfl_abbr_data['abbr'] == team_abbr].iloc[0]\n",
    "        df['game_type'] = \"regular season\"\n",
    "\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        print(f\"Error retrieving data for {team_abbr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through each nfl team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nfl_team in nfl_abbr_data['abbr']:\n",
    "    \n",
    "    # Scrape\n",
    "    nfl_team_sch = retrieve_nfl_schedule(nfl_team)\n",
    "\n",
    "    # Check for dataframe\n",
    "    if 'df_nfl_master' in locals():\n",
    "        df_nfl_master = pd.concat([df_nfl_master, nfl_team_sch], ignore_index=True)\n",
    "    else:\n",
    "        df_nfl_master = nfl_team_sch.copy()\n",
    "\n",
    "    # Pause for n seconds to address HTTPError: HTTP Error 429: Too Many Requests\n",
    "    time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_nfl_master.user_team.value_counts(), len(df_nfl_master.user_team.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nfl_master.to_csv('df_nfl_master_2024_10_31.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NFL (espn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import time\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_abbr_data = pd.read_csv(r\"C:\\Users\\Owner\\Documents\\Data Projects\\GitHub\\Apps\\project_w\\data\\nfl_espn_team_abbr.csv\")\n",
    "# today = datetime.now().date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build scraper fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the schedule from nba ref\n",
    "def retrieve_espn_nfl_schedule(team_abbr):\n",
    "\n",
    "    # URL of the website\n",
    "    url = f\"https://www.espn.com/nfl/team/schedule/_/name/{team_abbr}\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"YourAppName/1.0 (https://yourwebsite.com; contact@yourwebsite.com)\"\n",
    "    }\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        time.sleep(4) # if successful, wait n secs before reading in the html\n",
    "        df = pd.read_html(url)[0]\n",
    "\n",
    "        # Find the index of the row that starts with \"WK\", \"DATE\", etc.\n",
    "        start_row = df[(df[0] == \"WK\") & (df[1] == \"DATE\") & (df[2] == \"OPPONENT\") & (df[3] == \"TIME\")].index[0]\n",
    "\n",
    "        # Find the index of the row that starts with \"Preseason\"\n",
    "        end_row = df[df[0] == \"Preseason\"].index[0]\n",
    "\n",
    "        # Select rows between the start and end rows, and set the start_row as the header\n",
    "        df = df.iloc[(start_row-1) + 1:end_row].reset_index(drop=True)  # +1 to exclude the header row from the data\n",
    "        df.columns = df.iloc[0]  # Set the first row of the selection as column headers\n",
    "        df = df[1:]  # Drop the header row itself\n",
    "\n",
    "        # Filter out rows where any cell has \"BYE WEEK\"\n",
    "        df = df[~df.eq(\"BYE WEEK\").any(axis=1)]\n",
    "\n",
    "        # Drop the first column and the last two columns\n",
    "        df = df.iloc[:, :-2].reset_index(drop=True)\n",
    "\n",
    "        # Extract \"vs\" or \"@\" into a new column \"Home/Away\"\n",
    "        df['Home/Away'] = df['OPPONENT'].str.extract(r'(vs|@)')\n",
    "        df['Home/Away'] = df['Home/Away'].replace('vs', 'vs.')\n",
    "\n",
    "        # Remove \"vs\" or \"@\" from the \"OPPONENT\" column and strip whitespace\n",
    "        df['OPPONENT'] = df['OPPONENT'].str.replace(r'vs |@ ', '', regex=True).str.strip()\n",
    "\n",
    "        ## Organize\n",
    "        df['user_team'] = nfl_abbr_data['team'][nfl_abbr_data['abbr'] == team_abbr].iloc[0]\n",
    "        df['game_type'] = \"regular season\"\n",
    "        df = df.drop(columns=df.filter(regex=\"^tickets$\").columns)\n",
    "        df['TIME'] = df['TIME'].replace(\"TBD\", pd.NA)\n",
    "\n",
    "        # Replace \"TBD\" with NaN using loc\n",
    "        df.loc[df['DATE'].str.contains('TBD'), 'DATE'] = np.nan\n",
    "\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        print(f\"Error retrieving data for {team_abbr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through each nfl team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nfl_team in nfl_abbr_data['abbr']:\n",
    "    \n",
    "    # Scrape\n",
    "    nfl_team_sch = retrieve_espn_nfl_schedule(nfl_team)\n",
    "\n",
    "    # Check for dataframe\n",
    "    if 'df_nfl_master' in locals():\n",
    "        df_nfl_master = pd.concat([df_nfl_master, nfl_team_sch], ignore_index=True)\n",
    "    else:\n",
    "        df_nfl_master = nfl_team_sch.copy()\n",
    "\n",
    "    # Pause for n seconds to address HTTPError: HTTP Error 429: Too Many Requests\n",
    "    time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miami Dolphins           10\n",
      "Dallas Cowboys           10\n",
      "Kansas City Chiefs       10\n",
      "Philadelphia Eagles      10\n",
      "Detroit Lions            10\n",
      "Los Angeles Chargers     10\n",
      "Chicago Bears            10\n",
      "Los Angeles Rams         10\n",
      "Tennessee Titans         10\n",
      "Minnesota Vikings        10\n",
      "Seattle Seahawks          9\n",
      "San Francisco 49ers       9\n",
      "New England Patriots      9\n",
      "Pittsburgh Steelers       9\n",
      "Tampa Bay Buccaneers      9\n",
      "Las Vegas Raiders         9\n",
      "New York Giants           9\n",
      "New Orleans Saints        9\n",
      "Arizona Cardinals         9\n",
      "Atlanta Falcons           9\n",
      "Jacksonville Jaguars      9\n",
      "Indianapolis Colts        9\n",
      "Green Bay Packers         9\n",
      "Denver Broncos            9\n",
      "Cleveland Browns          9\n",
      "Cincinnati Bengals        9\n",
      "Carolina Panthers         9\n",
      "Buffalo Bills             9\n",
      "Baltimore Ravens          9\n",
      "Washington Commanders     9\n",
      "New York Jets             8\n",
      "Houston Texans            8\n",
      "Name: user_team, dtype: int64 32\n"
     ]
    }
   ],
   "source": [
    "print(df_nfl_master.user_team.value_counts(), len(df_nfl_master.user_team.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teams in `df_nfl_master.user_team` but not in `nfl_abbr_data.team`: set()\n",
      "Teams in `nfl_abbr_data.team` but not in `df_nfl_master.user_team`: set()\n"
     ]
    }
   ],
   "source": [
    "# Convert both arrays to sets\n",
    "user_team_set = set(df_nfl_master.user_team.unique())\n",
    "abbr_team_set = set(nfl_abbr_data.team.unique())\n",
    "\n",
    "# Find differences\n",
    "only_in_user_team = user_team_set - abbr_team_set\n",
    "only_in_abbr_team = abbr_team_set - user_team_set\n",
    "\n",
    "# Display results\n",
    "print(\"Teams in `df_nfl_master.user_team` but not in `nfl_abbr_data.team`:\", only_in_user_team)\n",
    "print(\"Teams in `nfl_abbr_data.team` but not in `df_nfl_master.user_team`:\", only_in_abbr_team)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nfl_master['Home/Away'] = df_nfl_master['Home/Away'].replace('vs', 'vs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nfl_master.to_csv('df_nfl_master_2024_11_01.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WK</th>\n",
       "      <th>DATE</th>\n",
       "      <th>OPPONENT</th>\n",
       "      <th>TIME</th>\n",
       "      <th>TV</th>\n",
       "      <th>Home/Away</th>\n",
       "      <th>user_team</th>\n",
       "      <th>game_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>Sun, Nov 3</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>4:05 PM</td>\n",
       "      <td>CBS</td>\n",
       "      <td>vs.</td>\n",
       "      <td>Arizona Cardinals</td>\n",
       "      <td>regular season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Sun, Nov 10</td>\n",
       "      <td>New York</td>\n",
       "      <td>4:25 PM</td>\n",
       "      <td>CBS</td>\n",
       "      <td>vs.</td>\n",
       "      <td>Arizona Cardinals</td>\n",
       "      <td>regular season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>Sun, Nov 24</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>4:25 PM</td>\n",
       "      <td>FOX</td>\n",
       "      <td>@</td>\n",
       "      <td>Arizona Cardinals</td>\n",
       "      <td>regular season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>Sun, Dec 1</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>1:00 PM</td>\n",
       "      <td>FOX</td>\n",
       "      <td>@</td>\n",
       "      <td>Arizona Cardinals</td>\n",
       "      <td>regular season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>Sun, Dec 8</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>4:05 PM</td>\n",
       "      <td>CBS</td>\n",
       "      <td>vs.</td>\n",
       "      <td>Arizona Cardinals</td>\n",
       "      <td>regular season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>13</td>\n",
       "      <td>Sun, Dec 1</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>1:00 PM</td>\n",
       "      <td>CBS</td>\n",
       "      <td>vs.</td>\n",
       "      <td>Washington Commanders</td>\n",
       "      <td>regular season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>15</td>\n",
       "      <td>Sun, Dec 15</td>\n",
       "      <td>New Orleans</td>\n",
       "      <td>1:00 PM</td>\n",
       "      <td>FOX</td>\n",
       "      <td>@</td>\n",
       "      <td>Washington Commanders</td>\n",
       "      <td>regular season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>16</td>\n",
       "      <td>Sun, Dec 22</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>1:00 PM</td>\n",
       "      <td>FOX</td>\n",
       "      <td>vs.</td>\n",
       "      <td>Washington Commanders</td>\n",
       "      <td>regular season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vs.</td>\n",
       "      <td>Washington Commanders</td>\n",
       "      <td>regular season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@</td>\n",
       "      <td>Washington Commanders</td>\n",
       "      <td>regular season</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>296 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0    WK         DATE      OPPONENT     TIME   TV Home/Away  \\\n",
       "0     9   Sun, Nov 3       Chicago  4:05 PM  CBS       vs.   \n",
       "1    10  Sun, Nov 10      New York  4:25 PM  CBS       vs.   \n",
       "2    12  Sun, Nov 24       Seattle  4:25 PM  FOX         @   \n",
       "3    13   Sun, Dec 1     Minnesota  1:00 PM  FOX         @   \n",
       "4    14   Sun, Dec 8       Seattle  4:05 PM  CBS       vs.   \n",
       "..   ..          ...           ...      ...  ...       ...   \n",
       "291  13   Sun, Dec 1     Tennessee  1:00 PM  CBS       vs.   \n",
       "292  15  Sun, Dec 15   New Orleans  1:00 PM  FOX         @   \n",
       "293  16  Sun, Dec 22  Philadelphia  1:00 PM  FOX       vs.   \n",
       "294  17          NaN       Atlanta     <NA>  NaN       vs.   \n",
       "295  18          NaN        Dallas     <NA>  NaN         @   \n",
       "\n",
       "0                user_team       game_type  \n",
       "0        Arizona Cardinals  regular season  \n",
       "1        Arizona Cardinals  regular season  \n",
       "2        Arizona Cardinals  regular season  \n",
       "3        Arizona Cardinals  regular season  \n",
       "4        Arizona Cardinals  regular season  \n",
       "..                     ...             ...  \n",
       "291  Washington Commanders  regular season  \n",
       "292  Washington Commanders  regular season  \n",
       "293  Washington Commanders  regular season  \n",
       "294  Washington Commanders  regular season  \n",
       "295  Washington Commanders  regular season  \n",
       "\n",
       "[296 rows x 8 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nfl_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
